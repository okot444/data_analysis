import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
'''
X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([0, 1, 0])
clf = DecisionTreeClassifier()
clf.fit(X, y)
print(clf)

importances = clf.feature_importances_
np.isnan(X)
print(importances)'''

# Загрузите выборку из файла titanic.csv с помощью пакета Pandas.
data = pd.read_csv('titanic.csv', index_col='PassengerId')

# Оставьте в выборке четыре признака: класс пассажира (Pclass), цену билета (Fare),
# возраст пассажира (Age) и его пол (Sex).

#  Столбцы таблицы, содержат независимые переменные (признаки), обозначаемые X,
# и зависимые (целевые) переменные, обозначаемые y.

# Целевые переменные определяют класс задачи и могут быть представлены одним из следующих вариантов:
#
#  Один столбец с двоичными значениями: задача двухклассовой классификации (binary classification),
# каждый объект принадлежит только одному классу.
# Один столбец с действительными значениями: задача регрессии, прогнозируется одна величина.
#  Несколько столбцов с двоичными значениями: задача многоклассовой классификации (multi-class classification),
# каждый объект принадлежит только одному классу.
#  Несколько столбцов с действительными значениями: задача регрессии, прогнозируется несколько величин.
#  Несколько столбцов с двоичными значениями: задача классификации с пересекающимися классами
# (multi-label classification), один объект может принадлежать нескольким классам.


data = data[['Pclass', 'Fare', 'Age', 'Sex', 'Survived']]
data = data.dropna()

X = data[['Pclass', 'Fare', 'Age', 'Sex']]
X.Sex.replace(to_replace=dict(female=1, male=0), inplace=True)


# 4. Выделите целевую переменную — она записана в столбце Survived.
y = data['Survived']


# 5. Найдите все объекты, у которых есть пропущенные признаки, и удалите их из выборки.
# Для того, чтобы удалить все объекты, которые содержат значения NaN воспользуйтесь
# методом dropna() без аргументов.
X = X.dropna()


# 6. Обучите решающее дерево с параметром random_state=241 и остальными параметрами по умолчанию
# (речь идет о параметрах конструктора DecisionTreeСlassifier).
clf = DecisionTreeClassifier(random_state=241)


# Обучение модели производится с помощью функции fit.
clf.fit(X, y)

# Вычислите важности признаков и найдите два признака с наибольшей важностью.
# Их названия будут ответами для данной задачи (в качестве ответа укажите названия
# признаков через запятую или пробел, порядок не важен).

importances = clf.feature_importances_
print(importances)


with open('7destree.txt', 'w') as f:
    f.write("Sex Fare")
f.close()